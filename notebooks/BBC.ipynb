{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC News Page\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this example we are going to have a look at the BBC News page with a view to extracting details of the top read and watched stories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Browser Inspector\n",
    "\n",
    "All modern browsers allow you to access the underlying HTML Code which makes up a Web page\n",
    "\n",
    "It is the job of the Browser to interpret the HTML and present the information it represents on the screen in a user friendly manner.\n",
    "\n",
    "In order to Web scrape, you do need to have some understanding of HTML but not a great deal. Like most coding languages it is easier to read than to write and we only need to be able to read it a little bit, e.g. recognise different components or tags and a bit about the syntax of tags. \n",
    "\n",
    "A more important requirement is to be able to match what we see on the screen with the underlying HTML. A thorough understanding of the HTML and CSS code will allow you to do this, but there is a far easier way.\n",
    "\n",
    "This involves using the developer tools found in all modern browsers and in particular the 'element inspector'. This allows you to select an element on the web page; a table, part of a table, a link, almost anything and have the corresponding HTML code highlighted.\n",
    "\n",
    "These developer tools are available in:\n",
    "\n",
    "* MS Edge\n",
    "* Chrome\n",
    "* Firefox\n",
    "\n",
    "and probably more modern Web Browsers.\n",
    "\n",
    "You will generally find the 'Developer Tools ' somewhere in the Browser Menu system. However (currently) the shortcut Ctrl + Shift + c works for all browsers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information that we might want to scrape and save\n",
    "\n",
    "### Using the BBC News page\n",
    "\n",
    "1. List of most read with links\n",
    "2. List of most watched with links\n",
    "\n",
    "### If we are going to accumulate data over time we will also want a timestamp which we will generate when we run the code and for each run and append the new data to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The packages we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs      # pip install beautifulsoup4 but import from bs4\n",
    "import time\n",
    "from datetime import datetime\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'get' methods from requests only needs to be given a string representing a url\n",
    "\n",
    "Quite often if you need to provide multiple parameters you would build the url string up and then call  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick example to show how Beautifulsoup works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.bbc.co.uk')\n",
    "#print(r.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can make the output look a bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(r.text)               \n",
    "prettyHTML = soup.prettify() \n",
    "#print(prettyHTML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can save the output to a file in this formatted way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('page.html', 'w', encoding='utf-8') as fw :\n",
    "    fw.write(prettyHTML)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `find` and `find_all` allow you to search for tags\n",
    "\n",
    "### `get` allows you to select parameter values or tag values\n",
    "\n",
    "### Typically we will be finding tags and then extracting values from them.\n",
    "\n",
    "### What we need to ensure when doing this is that we have selected the correct tags. In any given webpage some of the common tags will occur many times as we will see.\n",
    "\n",
    "### we can do this by either using a chain of tags which is unique and ends in the tag we want or make use of the parameters and values within a tag and find a unique combination which will identify the specific tag we want.\n",
    "\n",
    "### This is why we need to inspect the HTML in order to identify these unique combinations.\n",
    "\n",
    "### In HTML tags are written in a specific way "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can find all of the images within the Web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imagelink in soup.find_all('img'):\n",
    "    url = imagelink.get('src')\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can find all of the links within the Web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in soup.find_all('a') :\n",
    "    print(f\" {url.text} --> {url.get('href')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing to the BBC News page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.bbc.co.uk/news')\n",
    "soup = bs(r.text)               \n",
    "prettyHTML = soup.prettify() \n",
    "with open('news_page.html', 'w', encoding='utf-8') as fw :\n",
    "    fw.write(prettyHTML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about list items?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for list_items in soup.find_all('li'):\n",
    "    #print(f'{list_items} -->  ')\n",
    "    print(f'{list_items.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  I know from using the developer tools that I am interested in 'li' items with 'data-entityid' properties, so just list them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for list_items in soup.find_all('li'):\n",
    "    id = list_items.get('data-entityid')\n",
    "    if id is not None :\n",
    "        print(id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I want to further refine this to get rid of the social media items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for list_items in soup.find_all('li'):\n",
    "    id = list_items.get('data-entityid')\n",
    "    if id is not None :\n",
    "        if id[0:4] == 'most' :\n",
    "            print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But these do not appear to be simple list items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for list_items in soup.find_all('li'):\n",
    "    id = list_items.get('data-entityid')\n",
    "    if id is not None :\n",
    "        if id[0:4] == 'most' :\n",
    "            print(f'{list_items.text} -------> {list_items}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can use our saved file of HTML to help us find what we want. \n",
    "\n",
    "1. load the file into a text editor like Notepad++\n",
    "2. seach for the string 'data-entityid' and check you are in a 'li' item\n",
    "3. Within the structure of the 'li' we want to find the tags associated with the displayed text and the URL of the link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "          <li class=\"gel-layout__item gs-o-faux-block-link gs-u-mb+ gel-1/2@m gel-1/5@xxl gs-u-float-left@m gs-u-clear-left@m gs-u-float-none@xxl\" data-entityid=\"most-popular-read-1\">\n",
    "           <span class=\"gs-o-media\">\n",
    "            <span class=\"nw-c-most-read__rank gs-o-media__img gel-canon gel-1/12@xs gel-1/8@m gel-1/10@l gel-2/12@xxl gs-u-align-center\">\n",
    "             1\n",
    "            </span>\n",
    "            <div class=\"gs-o-media__body\">\n",
    "             <a class=\"gs-c-promo-heading nw-o-link gs-o-bullet__text gs-o-faux-block-link__overlay-link gel-pica-bold gs-u-pl-@xs\" href=\"/news/business-57712618\">\n",
    "              <span class=\"gs-c-promo-heading__title gel-pica-bold\">\n",
    "               John Lewis plans to build 10,000 rental homes\n",
    "              </span>\n",
    "             </a>\n",
    "            </div>\n",
    "           </span>\n",
    "          </li>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The URL has to specified as the href value in an \\<a\\> tag, so that is quite straight forward. But notice that it is a relative address. We will need to fix that later.\n",
    "5. The displayed text is within  \\<span\\>\\<\\\\span\\> tags of which there are several, so we need to find something that makes this <span> unique. The answer appears to be the class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for list_items in soup.find_all('li'):\n",
    "    id = list_items.get('data-entityid')\n",
    "    if id is not None :\n",
    "        if id[0:4] == 'most' :\n",
    "            print(id)\n",
    "            for anc in list_items.find('div') :\n",
    "                print(anc.get('href'))\n",
    "            for text in list_items.find('span',{'class' : 'gs-c-promo-heading__title'} ) :\n",
    "                print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Now put it all together\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First create our csv file with a header record\n",
    "\n",
    "#### We only need to run this once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output file\n",
    "\n",
    "with open('BBC_top_hits.csv', 'w', encoding = 'utf-8') as fw:\n",
    "    outfile = csv.writer(fw, delimiter=',', lineterminator='\\r')\n",
    "    outfile.writerow([\"Date\", \"Item_pos\", \"Title\", \"Link\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about getting our timestamp\n",
    "\n",
    "#### We will add a timestamp to each record we write to the file. This will allow us to show how the favourite change over time. To do this we use the now() method from the datetime package (this was loaded at the beginning of the notebook). The now() mthod returns a timestamp that you can print but is actually stored in an internal format. So to use it we use the strftime() method and pass it the format that we want returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current timestamp for file\n",
    "now = datetime.now()\n",
    "print(now)\n",
    "current_time = now.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "print(current_time)\n",
    "print(now[0:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up \n",
    "\n",
    "url_prefix = 'https://www.bbc.co.uk'\n",
    "\n",
    "r = requests.get('https://www.bbc.co.uk/news')\n",
    "#print(r.text)\n",
    "\n",
    "soup = bs(r.text)               \n",
    "#prettyHTML = soup.prettify() \n",
    "#print(prettyHTML)\n",
    "\n",
    "# The output file\n",
    "with open('BBC_top_hits.csv', 'a', encoding = 'utf-8') as fw:\n",
    "    outfile = csv.writer(fw, delimiter=',', lineterminator='\\r')\n",
    "\n",
    "    # get current timestamp fot file\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "    for list_items in soup.find_all('li'):\n",
    "        id = list_items.get('data-entityid')\n",
    "        if id is not None :\n",
    "            if id[0:4] == 'most' :\n",
    "                #print(id)\n",
    "                item_no = id\n",
    "                for anc in list_items.find('div') :\n",
    "                    #print(anc.get('href'))\n",
    "                    href = anc.get('href')\n",
    "                    full_url = url_prefix + href\n",
    "                    \n",
    "                for text in list_items.find('span',{'class' : 'gs-c-promo-heading__title'} ) :\n",
    "                    #print(text)\n",
    "                    print(f'{current_time},{item_no},{text}, {full_url}')\n",
    "                    outfile.writerow([current_time,item_no,text, full_url])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code above works so we could stop at this point, but as we have isolated the text and the URL that we want uniquely, we can rewrite the code without the `for` loops.\n",
    "### We do however need to explicitly extract the `a` tag from the `div` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up \n",
    "\n",
    "url_prefix = 'https://www.bbc.co.uk'\n",
    "\n",
    "r = requests.get('https://www.bbc.co.uk/news')\n",
    "#print(r.text)\n",
    "\n",
    "soup = bs(r.text)               \n",
    "#prettyHTML = soup.prettify() \n",
    "#print(prettyHTML)\n",
    "\n",
    "# The output file\n",
    "with open('BBC_top_hits.csv', 'a', encoding = 'utf-8') as fw:\n",
    "    outfile = csv.writer(fw, delimiter=',', lineterminator='\\r')\n",
    "\n",
    "    # get current timestamp fot file\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "    for list_items in soup.find_all('li'):\n",
    "        id = list_items.get('data-entityid')\n",
    "        if id is not None :\n",
    "            if id[0:4] == 'most' :\n",
    "                item_no = id\n",
    "                \n",
    "                anc = list_items.find('div')\n",
    "                a_tag = anc.find('a')\n",
    "                href = a_tag.get('href')\n",
    "                full_url = url_prefix + href\n",
    "                    \n",
    "                span = list_items.find('span',{'class' : 'gs-c-promo-heading__title'} )\n",
    "                text = span.text\n",
    "\n",
    "                print(f'{current_time},{item_no},{text}, {full_url}')\n",
    "                outfile.writerow([current_time,item_no,text, full_url])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
